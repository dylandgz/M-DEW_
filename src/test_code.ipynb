{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for Age: 18.620408\n",
      "Mean Squared Error for Salary: 14038580.899435747\n",
      "Accuracy for Gender: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dylandominguez/StudioProjects/M-DEW/.venv/lib/python3.8/site-packages/sklearn/impute/_iterative.py:800: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "# Step 1: Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "data = {\n",
    "    'Age': np.random.randint(20, 60, n_samples),\n",
    "    'Salary': np.random.normal(50000, 12000, n_samples),\n",
    "    'Gender': np.random.choice(['Male', 'Female'], n_samples)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Step 2: Introduce missing data\n",
    "missing_rate = 0.1\n",
    "for col in df.columns:\n",
    "    n_missing = int(missing_rate * n_samples)\n",
    "    missing_indices = np.random.choice(df.index, n_missing, replace=False)\n",
    "    df.loc[missing_indices, col] = np.nan\n",
    "\n",
    "# Step 3: Impute missing data\n",
    "# Encode categorical data\n",
    "encoder = LabelEncoder()\n",
    "encoded_genders = encoder.fit_transform(df['Gender'].astype(str))\n",
    "df['Gender'] = encoded_genders\n",
    "\n",
    "# Using Iterative Imputer for a more robust imputation\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(), random_state=42, max_iter=10)\n",
    "imputed_numerical = imputer.fit_transform(df[['Age', 'Salary']])\n",
    "\n",
    "# Using a simple imputer for the categorical 'Gender' as iterative imputer might not directly apply\n",
    "simple_imputer = SimpleImputer(strategy='most_frequent')\n",
    "imputed_genders = simple_imputer.fit_transform(df[['Gender']])\n",
    "\n",
    "# Combine imputed data\n",
    "df_imputed = pd.DataFrame(np.column_stack([imputed_numerical, imputed_genders]), columns=df.columns)\n",
    "df_imputed['Gender'] = encoder.inverse_transform(df_imputed['Gender'].astype(int))\n",
    "\n",
    "# Step 4: Evaluate imputation\n",
    "# For simplicity, assume we somehow know original data (this part is hypothetical and just for practice)\n",
    "original_data = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Calculate MSE for numerical data\n",
    "mse_age = mean_squared_error(original_data['Age'], df_imputed['Age'])\n",
    "mse_salary = mean_squared_error(original_data['Salary'], df_imputed['Salary'])\n",
    "\n",
    "# Calculate accuracy for categorical data\n",
    "accuracy_gender = accuracy_score(original_data['Gender'], df_imputed['Gender'])\n",
    "\n",
    "# Display results\n",
    "print(f\"Mean Squared Error for Age: {mse_age}\")\n",
    "print(f\"Mean Squared Error for Salary: {mse_salary}\")\n",
    "print(f\"Accuracy for Gender: {accuracy_gender:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Traceback (most recent call last):\n",
    "  File \"experiment.py\", line 1046, in <module>\n",
    "    data_preparation_function_object = CURRENT_SUPPORTED_DATALOADERS[dataset]\n",
    "  File \"experiment.py\", line 997, in run\n",
    "    \n",
    "  File \"experiment.py\", line 766, in run_custom_experiments\n",
    "    imputed_X_train_df = pd.DataFrame(experiment.imputed_X_train, columns=data_copy_X.columns).reset_index(drop=True)\n",
    "  File \"/Users/dylandominguez/StudioProjects/M-DEW/.venv/lib/python3.8/site-packages/pandas/core/frame.py\", line 758, in __init__\n",
    "    mgr = ndarray_to_mgr(\n",
    "  File \"/Users/dylandominguez/StudioProjects/M-DEW/.venv/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 337, in ndarray_to_mgr\n",
    "    _check_values_indices_shape_match(values, index, columns)\n",
    "  File \"/Users/dylandominguez/StudioProjects/M-DEW/.venv/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 408, in _check_values_indices_shape_match\n",
    "    raise ValueError(f\"Shape of passed values is {passed}, indices imply {implied}\")\n",
    "ValueError: Shape of passed values is (119, 13), indices imply (119, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#below is the code that was used before. not working\n",
    "\n",
    "with tqdm(total=len(miss_param_grid)) as pbar:\n",
    "        for i, params in enumerate(miss_param_grid):\n",
    "            print(f\"\\nStarting experiment with params: {params}\")  # Print current experiment parameters\n",
    "            data_copy = deepcopy(data)\n",
    "            params = {\n",
    "                k: p\n",
    "                for k, p in zip(list(miss_param_dict.keys()), params)\n",
    "            }\n",
    "            param_lookup_dict[i] = params\n",
    "            \n",
    "            # imputation\n",
    "            dataset = MissDataset(\n",
    "                data=data_copy,\n",
    "                target_col=target_col,\n",
    "                n_folds=5,\n",
    "                **params,\n",
    "            )\n",
    "            if dataset_name == 'parkinsons':\n",
    "                dataset.split_dataset_hook(\n",
    "                    splitting_function=split_parkinsons_data, df=dataset.data, n_folds=5\n",
    "                )\n",
    "            if task_type == 'classification':\n",
    "                experiment = CustomClassificationExperiment(\n",
    "                    dataset=dataset, dataset_name=dataset_name,\n",
    "                    exp_type=params['missing_mechanism'],\n",
    "                    name=name\n",
    "                )\n",
    "            else:\n",
    "                experiment = CustomRegressionExperiment(\n",
    "                    dataset=dataset, dataset_name=dataset_name,\n",
    "                    exp_type=params['missing_mechanism'],\n",
    "                    name=name\n",
    "                )\n",
    "            metrics_df, errors_df, weights_dfs, preds_df, distances_df = experiment.run()\n",
    "            ##############################\n",
    "            \n",
    "\n",
    "\n",
    "            # # Evaluate imputation quality\n",
    "            # for col in data_copy.columns:\n",
    "            #     if col != target_col:  # Skip the target column for imputation evaluation\n",
    "            #         if data_copy[col].dtype.kind in 'iufc':  # integer, unsigned integer, float, complex considered as continuous\n",
    "            #             original_data = data_copy[col].dropna()\n",
    "            #             imputed_data = dataset.data.loc[original_data.index, col]\n",
    "            #             rmse = np.sqrt(mean_squared_error(original_data, imputed_data))\n",
    "            #             imputation_eval_results.append({\"column\": col, \"metric\": \"RMSE\", \"value\": rmse})\n",
    "            #             print(f\"RMSE for {col} with params {params}: {rmse}\")\n",
    "            #         else:  # Assume non-numeric data types are categorical\n",
    "            #             original_data = data_copy[col].dropna()\n",
    "            #             imputed_data = dataset.data.loc[original_data.index, col]\n",
    "            #             if len(set(original_data)) > 1:  # ROC AUC requires at least two classes\n",
    "            #                 auroc = roc_auc_score(original_data.astype('category').cat.codes, imputed_data.astype('category').cat.codes)\n",
    "            #                 imputation_eval_results.append({\"column\": col, \"metric\": \"AUROC\", \"value\": auroc})\n",
    "            #                 print(f\"AUROC for {col} with params {params}: {auroc}\")\n",
    "\n",
    "\n",
    "    #         # Collect imputed data and evaluate\n",
    "    #         data_copy_X_before = data_copy.drop(columns=[target_col]).reset_index(drop=True)\n",
    "    #         X = data_copy.drop(columns=[target_col])\n",
    "    #         scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #         data_copy_X = scaler.fit_transform(X)\n",
    "    #         data_copy_X= pd.DataFrame(data_copy_X, columns=data_copy_X_before.columns).reset_index(drop=True)\n",
    "\n",
    "    #         # imputed_X_train_df = pd.DataFrame(experiment.imputed_X_train, columns=data_copy_X.columns).reset_index(drop=True)\n",
    "    #         # imputed_X_test_df = pd.DataFrame(pipeline.last_imputed_X_test, columns=data_copy_X.columns).reset_index(drop=True)\n",
    "    #         # # imputed_X_val_df = pd.DataFrame(experiment.imputed_X_val, columns=data_copy_X.columns).reset_index(drop=True)\n",
    "    #         # data_copy_X = experiment.dataset.data.drop(columns=[target_col]).reset_index(drop=True)\n",
    "            \n",
    "            \n",
    "    #         for pipeline_name, pipeline in experiment.pipelines.items():\n",
    "    #             print(\"Original Data:\")\n",
    "    #             print(data_copy_X)\n",
    "\n",
    "\n",
    "    #             # print(type(experiment.imputed_X_val))\n",
    "    #             # print(experiment.imputed_X_val.shape)\n",
    "\n",
    "    #                 # Impute the data and directly convert it to a DataFrame\n",
    "    #             imputed_array = pipeline.imputer.fit_transform(data_copy_X)\n",
    "    #             imputed_data_df = pd.DataFrame(imputed_array, columns=data_copy_X.columns)\n",
    "                    \n",
    "                \n",
    "                \n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    #             # imputed_data_df = pd.DataFrame(pipeline.last_imputed_X_, columns=data_copy_X.columns).reset_index(drop=True)\n",
    "    #             print(\"Imputed Data:\")\n",
    "    #             print(imputed_data_df)\n",
    "    #             print(\"+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    #             print(data_copy_X.isnull().sum())\n",
    "    #             print(\"This is the imputed data missing values:\")\n",
    "    #             print(imputed_data_df.isnull().sum())\n",
    "\n",
    "    #             print(f\"\\nRunning pipeline: {pipeline_name}\")\n",
    "    #             for col in data_copy_X.columns:\n",
    "    #                 if col != target_col:  # Skip target column evaluation\n",
    "    #                     original_col_data = data_copy_X[col].dropna()\n",
    "    #                     imputed_col_data = imputed_data_df[col].loc[original_col_data.index]\n",
    "    #                     if np.issubdtype(data_copy_X[col].dtype, np.number):\n",
    "    #                         rmse = np.sqrt(mean_squared_error(original_col_data, imputed_col_data))\n",
    "    #                         result = {'pipeline': pipeline_name, 'feature': col, 'metric': 'RMSE', 'value': rmse}\n",
    "    #                     else:\n",
    "    #                         le = LabelEncoder()\n",
    "    #                         le.fit(np.concatenate([original_col_data, imputed_col_data]))\n",
    "    #                         encoded_original = le.transform(original_col_data)\n",
    "    #                         encoded_imputed = le.transform(imputed_col_data)\n",
    "    #                         auroc = roc_auc_score(encoded_original, encoded_imputed)\n",
    "    #                         result = {'pipeline': pipeline_name, 'feature': col, 'metric': 'AUROC', 'value': auroc}\n",
    "    #                     imputation_eval_results.append(result)\n",
    "\n",
    "\n",
    "\n",
    "    #         # Save imputation evaluation results per pipeline to CSV dylan\n",
    "    #         imputation_eval_df = pd.DataFrame(imputation_eval_results)\n",
    "    #         imputation_eval_filename = os.path.join(experiment.results_dir, f'imputation_eval_results_{i}.csv')\n",
    "    #         imputation_eval_df.to_csv(imputation_eval_filename, index=False)\n",
    "            \n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #         filename = str(i) + '.csv'\n",
    "    #         errors_filename = os.path.join(experiment.results_dir, 'errors_' + filename)\n",
    "    #         errors_df.to_csv(errors_filename)\n",
    "\n",
    "    #         metrics_dfs.append(metrics_df)\n",
    "    #         metrics_filename = os.path.join(experiment.results_dir, 'metrics_' + filename)\n",
    "    #         metrics_df.to_csv(metrics_filename)\n",
    "\n",
    "    #         preds_filename = os.path.join(experiment.results_dir, 'predictions_' + filename)\n",
    "    #         preds_df.to_csv(preds_filename)\n",
    "\n",
    "    #         for top_n in weights_dfs.keys():\n",
    "    #             weights_filename = os.path.join(experiment.results_dir, 'weights_top_' + str(top_n) + '_' + filename)\n",
    "    #             weights_dfs[top_n].to_csv(weights_filename)\n",
    "\n",
    "    #         distances_filename = os.path.join(experiment.results_dir, 'distances_' + filename)\n",
    "    #         distances_df.to_csv(distances_filename)\n",
    "    #         print('updating progress bar after index ' + str(i))\n",
    "    #         pbar.update(1)\n",
    "\n",
    "    # # Save final imputation evaluation results for all pipelines dylan\n",
    "    # final_imputation_eval_df = pd.DataFrame(imputation_eval_results)  # NEW: Aggregated results\n",
    "    # final_imputation_eval_filename = os.path.join(experiment.results_dir, 'final_imputation_eval_results.csv')  # NEW: Filename for aggregated results\n",
    "    # final_imputation_eval_df.to_csv(final_imputation_eval_filename, index=False)  # NEW: Save aggregated results\n",
    "\n",
    "\n",
    "    final_results = pd.concat(metrics_dfs)\n",
    "    # print(f\"Metrics {metrics_dfs}\")\n",
    "    final_results.to_csv(os.path.join(experiment.base_dir, 'final_results.csv'))\n",
    "\n",
    "    param_lookup_dict_json = json.dumps(param_lookup_dict)\n",
    "    with open(os.path.join(experiment.base_dir, 'params_lookup.json'), 'w') as f:\n",
    "        f.write(param_lookup_dict_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_one_pipeline(self, pipeline, pipeline_name, X_train, y_train, X_test, y_test):\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        proba_predictions = pipeline.predict_proba(X_test)[:, 1]\n",
    "        if isinstance(proba_predictions, list):\n",
    "            proba_predictions = proba_predictions[0]\n",
    "\n",
    "        y_test_2d = np.array(self.label_enc.transform(y_test.reshape(-1, 1)).todense())\n",
    "        errors = np.abs(y_test - proba_predictions)\n",
    "        predictions = np.round(proba_predictions)\n",
    "        single_label_y_test = np.argmax(y_test_2d, axis=1)\n",
    "        roc_auc = roc_auc_score(y_test, proba_predictions)\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['roc_auc'] = round(roc_auc, 4)\n",
    "        accuracy = 1 - (np.sum(np.logical_xor(predictions, single_label_y_test)) / len(predictions))\n",
    "        metrics['accuracy'] = round(accuracy, 4)\n",
    "        metrics['f1_score'] = f1_score(single_label_y_test, predictions)\n",
    "        self.metrics[pipeline_name].append(list(metrics.values()))\n",
    "\n",
    "        return proba_predictions, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    def _run_one_pipeline(self, pipeline, pipeline_name, X_train, y_train, X_test, y_test):\n",
    "        \n",
    "        print(pipeline_name)\n",
    "        if pipeline_name !=\"<class 'sklearn.ensemble._stacking.StackingClassifier'>\":\n",
    "            ####This is the problem \n",
    "\n",
    "\n",
    "            # print(\"X_train shape\")\n",
    "            # print(X_train.shape)\n",
    "            # print(\"X_test shape\")\n",
    "            # print(X_test.shape)\n",
    "\n",
    "\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            X_train_imputed=pipeline.X_train_imputed\n",
    "            # print(\"X_imputed\")\n",
    "            # print(X_imputed)\n",
    "            \n",
    "            proba_predictions = pipeline.predict_proba(X_test)[:, 1]\n",
    "            X_test_imputed=pipeline.X_test_imputed\n",
    "\n",
    "            # print(\"Train not missing\")\n",
    "            # print(self.train_not_missing.shape)\n",
    "            # print(\"X_train_imputed\")\n",
    "            # print(X_train_imputed.shape)\n",
    "\n",
    "\n",
    "\n",
    "            # # print(\"Test not missing\")\n",
    "            # # print(self.test_not_missing.shape)\n",
    "            # print(\"X_test_imputed\")\n",
    "            # print(X_test_imputed.shape)\n",
    "\n",
    "\n",
    "            X_train_imputed=pd.DataFrame(X_train_imputed, columns=self.train_not_missing.columns, index=self.train_not_missing.index)\n",
    "            X_test_imputed=pd.DataFrame(X_test_imputed, columns=self.test_not_missing.columns, index=self.test_not_missing.index)\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "            if isinstance(proba_predictions, list):\n",
    "                proba_predictions = proba_predictions[0]\n",
    "\n",
    "\n",
    "            ###################\n",
    "            # Dylan added Combine imputed training and testing data for evaluation\n",
    "            X_combined_imputed = np.vstack((X_train_imputed, X_test_imputed))\n",
    "            X_combined_original = pd.concat([self.train_not_missing, self.test_not_missing]).reset_index(drop=True)\n",
    "\n",
    "            # Create masks for original missing data locations in train and test\n",
    "            missing_mask_train = X_train.isnull()\n",
    "            missing_mask_test = X_test.isnull()\n",
    "            missing_mask_combined = pd.concat([missing_mask_train, missing_mask_test]).reset_index(drop=True)\n",
    "\n",
    "            imputed_RMSE={}\n",
    "            imputed_roc_auc={}\n",
    "\n",
    "            # Evaluate imputation on numeric and categorical data separately\n",
    "            for col in X_combined_original.columns:\n",
    "                if X_combined_original[col].dtype.kind in 'iuf':\n",
    "                    # Numeric evaluation: RMSE\n",
    "                    # print(\"Numeric evaluation: RMSE\")\n",
    "                    original_data = X_combined_original[col][missing_mask_combined[col]]\n",
    "                    imputed_data = X_combined_imputed[:, X_combined_original.columns.get_loc(col)][missing_mask_combined[col]]\n",
    "                    if not original_data.empty:\n",
    "                        \n",
    "                        rmse = np.sqrt(mean_squared_error(original_data, imputed_data))\n",
    "                        \n",
    "                        imputed_RMSE[col] = rmse\n",
    "                else:\n",
    "                    # Categorical evaluation: ROC AUC\n",
    "                    # print(\"Categorical evaluation: ROC AUC\")\n",
    "                    original_data = X_combined_original[col][missing_mask_combined[col]].dropna()\n",
    "                    imputed_data = X_combined_imputed[:, X_combined_original.columns.get_loc(col)][missing_mask_combined[col]]\n",
    "                    imputed_data = pd.Series(imputed_data).dropna()\n",
    "                    if not original_data.empty and len(original_data.unique()) > 1:\n",
    "                        roc_auc = roc_auc_score(original_data.astype('category').cat.codes, imputed_data.astype('category').cat.codes)\n",
    "                        imputed_roc_auc[col] = roc_auc\n",
    "\n",
    "            ###################\n",
    "            # Dylan added imputed evaluations\n",
    "            # imputed_evals = {}\n",
    "            # imputed_evals['RMSE'] = round(imputed_RMSE, 4)\n",
    "            # imputed_evals['AUC_ROC'] = round(imputed_roc_auc, 4)\n",
    "            # self.imputed_evals[pipeline_name].append(list(imputed_evals.values()))\n",
    "\n",
    "            # Compile imputed data evaluations\n",
    "            imputed_evals = {\n",
    "                'RMSE': imputed_RMSE,\n",
    "                'AUC_ROC': imputed_roc_auc\n",
    "            }\n",
    "            if not hasattr(self, 'imputed_evals'):\n",
    "                self.imputed_evals = {}\n",
    "            if pipeline_name not in self.imputed_evals:\n",
    "                self.imputed_evals[pipeline_name] = []\n",
    "            self.imputed_evals[pipeline_name].append(imputed_evals)\n",
    "            # print(self.imputed_evals)\n",
    "\n",
    "            ###################\n",
    "\n",
    "\n",
    "            ####################\n",
    "        else:\n",
    "            pipeline.fit(X_train, y_train)\n",
    "            proba_predictions = pipeline.predict_proba(X_test)[:, 1]\n",
    "            if isinstance(proba_predictions, list):\n",
    "                proba_predictions = proba_predictions[0]\n",
    "                \n",
    "\n",
    "        y_test_2d = np.array(self.label_enc.transform(y_test.reshape(-1, 1)).todense())\n",
    "        errors = np.abs(y_test - proba_predictions)\n",
    "        predictions = np.round(proba_predictions)\n",
    "        single_label_y_test = np.argmax(y_test_2d, axis=1)\n",
    "        roc_auc = roc_auc_score(y_test, proba_predictions)\n",
    "\n",
    "        \n",
    "\n",
    "        metrics = {}\n",
    "        metrics['roc_auc'] = round(roc_auc, 4)\n",
    "        accuracy = 1 - (np.sum(np.logical_xor(predictions, single_label_y_test)) / len(predictions))\n",
    "        metrics['accuracy'] = round(accuracy, 4)\n",
    "        metrics['f1_score'] = f1_score(single_label_y_test, predictions)\n",
    "        self.metrics[pipeline_name].append(list(metrics.values()))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        return proba_predictions, errors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
